"""
Evaluation Framework for Retail Insights Assistant
Measures accuracy, faithfulness, relevance, and overall quality
"""
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import re
import json
import pandas as pd


@dataclass
class EvaluationResult:
    """Result of an evaluation run"""
    accuracy_score: float
    faithfulness_score: float
    relevance_score: float
    completeness_score: float
    overall_score: float
    details: Dict[str, Any]
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class TestCase:
    """A test case for evaluation"""
    question: str
    expected_sql_patterns: List[str]  # Regex patterns that should appear in SQL
    expected_columns: List[str]  # Columns expected in result
    expected_result_type: str  # 'single_value', 'list', 'comparison', 'trend'
    expected_keywords: List[str]  # Keywords expected in response
    min_rows: int = 0
    max_rows: int = 1000


class QueryEvaluator:
    """Evaluates query generation accuracy"""
    
    def __init__(self):
        self.test_cases = self._create_test_cases()
    
    def _create_test_cases(self) -> List[TestCase]:
        """Create standard test cases"""
        return [
            TestCase(
                question="What is the total revenue?",
                expected_sql_patterns=[r'SUM\s*\(\s*revenue\s*\)', r'SELECT'],
                expected_columns=['total', 'revenue', 'sum'],
                expected_result_type='single_value',
                expected_keywords=['revenue', 'total', '₹', 'INR'],
                max_rows=1
            ),
            TestCase(
                question="Top 5 states by revenue",
                expected_sql_patterns=[r'GROUP\s+BY\s+state', r'ORDER\s+BY.*DESC', r'LIMIT\s+5'],
                expected_columns=['state', 'revenue'],
                expected_result_type='list',
                expected_keywords=['state', 'revenue', 'top'],
                min_rows=1,
                max_rows=5
            ),
            TestCase(
                question="Monthly revenue trend",
                expected_sql_patterns=[r'GROUP\s+BY.*month', r'ORDER\s+BY'],
                expected_columns=['month', 'revenue'],
                expected_result_type='trend',
                expected_keywords=['month', 'revenue', 'trend'],
                min_rows=1
            ),
            TestCase(
                question="Compare B2B and B2C sales",
                expected_sql_patterns=[r'is_b2b|b2b', r'GROUP\s+BY'],
                expected_columns=['revenue', 'orders'],
                expected_result_type='comparison',
                expected_keywords=['B2B', 'B2C', 'compare', 'revenue'],
                min_rows=2,
                max_rows=2
            ),
            TestCase(
                question="What is the cancellation rate?",
                expected_sql_patterns=[r'cancel|is_cancelled', r'COUNT|SUM'],
                expected_columns=['rate', 'cancelled', 'total'],
                expected_result_type='single_value',
                expected_keywords=['%', 'cancellation', 'rate'],
                max_rows=1
            ),
            TestCase(
                question="Top categories by profit",
                expected_sql_patterns=[r'GROUP\s+BY\s+category', r'profit', r'ORDER\s+BY.*DESC'],
                expected_columns=['category', 'profit'],
                expected_result_type='list',
                expected_keywords=['category', 'profit', 'top'],
                min_rows=1
            ),
        ]
    
    def evaluate_sql(self, generated_sql: str, test_case: TestCase) -> Dict[str, Any]:
        """
        Evaluate generated SQL against expected patterns
        
        Args:
            generated_sql: The SQL query generated by the agent
            test_case: Test case with expected patterns
            
        Returns:
            Evaluation result with score and details
        """
        if not generated_sql:
            return {"score": 0.0, "matched": [], "missing": test_case.expected_sql_patterns}
        
        sql_upper = generated_sql.upper()
        matched = []
        missing = []
        
        for pattern in test_case.expected_sql_patterns:
            if re.search(pattern, sql_upper, re.IGNORECASE):
                matched.append(pattern)
            else:
                missing.append(pattern)
        
        score = len(matched) / len(test_case.expected_sql_patterns) if test_case.expected_sql_patterns else 1.0
        
        return {
            "score": score,
            "matched": matched,
            "missing": missing,
            "sql": generated_sql
        }
    
    def evaluate_result(self, df: pd.DataFrame, test_case: TestCase) -> Dict[str, Any]:
        """
        Evaluate query result against expectations
        
        Args:
            df: Query result DataFrame
            test_case: Test case with expectations
            
        Returns:
            Evaluation result with score and details
        """
        if df is None or df.empty:
            if test_case.min_rows == 0:
                return {"score": 1.0, "message": "Empty result is acceptable"}
            return {"score": 0.0, "message": "Expected results but got empty"}
        
        issues = []
        
        # Check row count
        if len(df) < test_case.min_rows:
            issues.append(f"Expected at least {test_case.min_rows} rows, got {len(df)}")
        if len(df) > test_case.max_rows:
            issues.append(f"Expected at most {test_case.max_rows} rows, got {len(df)}")
        
        # Check for expected columns (flexible matching)
        found_columns = 0
        df_columns_lower = [c.lower() for c in df.columns]
        
        for expected_col in test_case.expected_columns:
            for actual_col in df_columns_lower:
                if expected_col.lower() in actual_col or actual_col in expected_col.lower():
                    found_columns += 1
                    break
        
        column_score = found_columns / len(test_case.expected_columns) if test_case.expected_columns else 1.0
        
        # Calculate overall score
        row_score = 1.0 if not issues else 0.5
        score = (column_score + row_score) / 2
        
        return {
            "score": score,
            "row_count": len(df),
            "columns": list(df.columns),
            "column_score": column_score,
            "issues": issues
        }


class FaithfulnessEvaluator:
    """Evaluates if response is faithful to source data"""
    
    def evaluate(self, response: str, facts: List[Dict], 
                 source_data: pd.DataFrame) -> Dict[str, Any]:
        """
        Evaluate faithfulness of response to source data
        
        Args:
            response: Generated response text
            facts: Extracted facts from data
            source_data: Source DataFrame
            
        Returns:
            Faithfulness evaluation result
        """
        if not response:
            return {"score": 0.0, "message": "Empty response"}
        
        # Extract claims from response
        claims = self._extract_claims(response)
        
        # Verify each claim
        verified = 0
        unverified = []
        
        for claim in claims:
            if self._verify_claim(claim, facts, source_data):
                verified += 1
            else:
                unverified.append(claim)
        
        score = verified / len(claims) if claims else 1.0
        
        return {
            "score": score,
            "total_claims": len(claims),
            "verified_claims": verified,
            "unverified_claims": unverified
        }
    
    def _extract_claims(self, response: str) -> List[Dict]:
        """Extract factual claims from response"""
        claims = []
        
        # Extract number claims
        number_pattern = re.compile(r'(?:is|was|are|were|totals?|equals?)\s*[\$₹]?\s*([\d,]+\.?\d*)')
        for match in number_pattern.finditer(response):
            claims.append({
                "type": "number",
                "value": float(match.group(1).replace(',', '')),
                "context": match.group(0)
            })
        
        # Extract percentage claims
        pct_pattern = re.compile(r'(\d+\.?\d*)\s*%')
        for match in pct_pattern.finditer(response):
            claims.append({
                "type": "percentage",
                "value": float(match.group(1)),
                "context": match.group(0)
            })
        
        # Extract ranking claims
        ranking_pattern = re.compile(r'(highest|lowest|top|best|worst|most|least)\s+(\w+)')
        for match in ranking_pattern.finditer(response.lower()):
            claims.append({
                "type": "ranking",
                "direction": match.group(1),
                "metric": match.group(2),
                "context": match.group(0)
            })
        
        return claims
    
    def _verify_claim(self, claim: Dict, facts: List[Dict], 
                      source_data: pd.DataFrame) -> bool:
        """Verify a single claim against facts and source data"""
        
        if claim["type"] == "number":
            # Check if number exists in facts or data
            target = claim["value"]
            
            # Check facts
            for fact in facts:
                if "value" in fact and isinstance(fact["value"], (int, float)):
                    if abs(fact["value"] - target) / max(target, 1) < 0.05:  # 5% tolerance
                        return True
            
            # Check source data
            for col in source_data.select_dtypes(include=['number']).columns:
                values = [source_data[col].sum(), source_data[col].mean(), 
                         source_data[col].max(), source_data[col].min()]
                for v in values:
                    if abs(v - target) / max(target, 1) < 0.05:
                        return True
        
        elif claim["type"] == "ranking":
            # Verify ranking claims
            return True  # Simplified - would need more context
        
        return False


class RelevanceEvaluator:
    """Evaluates if response is relevant to the question"""
    
    def __init__(self):
        self.question_keywords = {
            "what": ["is", "are", "was", "were"],
            "how": ["much", "many"],
            "which": ["top", "best", "highest"],
            "compare": ["vs", "versus", "compared"],
            "trend": ["over", "time", "monthly", "yearly"]
        }
    
    def evaluate(self, question: str, response: str) -> Dict[str, Any]:
        """
        Evaluate if response answers the question
        
        Args:
            question: Original question
            response: Generated response
            
        Returns:
            Relevance evaluation result
        """
        if not response:
            return {"score": 0.0, "message": "Empty response"}
        
        question_lower = question.lower()
        response_lower = response.lower()
        
        # Extract key entities from question
        question_entities = self._extract_entities(question_lower)
        
        # Check if response addresses these entities
        addressed = 0
        for entity in question_entities:
            if entity in response_lower:
                addressed += 1
        
        entity_coverage = addressed / len(question_entities) if question_entities else 1.0
        
        # Check for answer indicators
        has_answer = self._check_answer_indicators(question_lower, response_lower)
        
        # Check response length (too short or too long)
        length_score = self._evaluate_length(question, response)
        
        overall_score = (entity_coverage * 0.4 + has_answer * 0.4 + length_score * 0.2)
        
        return {
            "score": overall_score,
            "entity_coverage": entity_coverage,
            "has_answer": has_answer,
            "length_score": length_score,
            "entities_found": question_entities
        }
    
    def _extract_entities(self, text: str) -> List[str]:
        """Extract key entities from text"""
        entities = []
        
        # Common retail entities
        retail_terms = ['revenue', 'sales', 'profit', 'orders', 'customers', 
                       'products', 'categories', 'states', 'regions']
        
        for term in retail_terms:
            if term in text:
                entities.append(term)
        
        # Extract quoted terms
        quoted = re.findall(r'"([^"]+)"', text)
        entities.extend(quoted)
        
        return entities
    
    def _check_answer_indicators(self, question: str, response: str) -> float:
        """Check if response contains answer indicators"""
        
        # For "what" questions, expect numbers or definitions
        if question.startswith('what'):
            if re.search(r'\d+', response):
                return 1.0
        
        # For "how much/many" questions, expect quantities
        if 'how much' in question or 'how many' in question:
            if re.search(r'[\$₹]?\s*[\d,]+', response):
                return 1.0
        
        # For "which" questions, expect lists or specific items
        if question.startswith('which'):
            if re.search(r'(top|highest|best|1\.|first)', response):
                return 1.0
        
        # For comparison questions, expect comparisons
        if 'compare' in question or 'vs' in question:
            if 'compared' in response or 'higher' in response or 'lower' in response:
                return 1.0
        
        return 0.5  # Partial credit
    
    def _evaluate_length(self, question: str, response: str) -> float:
        """Evaluate response length appropriateness"""
        q_len = len(question)
        r_len = len(response)
        
        # Response should be reasonably longer than question
        if r_len < q_len:
            return 0.5
        
        # But not excessively long
        if r_len > q_len * 20:
            return 0.7
        
        return 1.0


class CompletenessEvaluator:
    """Evaluates if response is complete"""
    
    def evaluate(self, question: str, response: str, 
                 data_available: bool = True) -> Dict[str, Any]:
        """
        Evaluate response completeness
        
        Args:
            question: Original question
            response: Generated response
            data_available: Whether data was available for the query
            
        Returns:
            Completeness evaluation result
        """
        if not response:
            return {"score": 0.0, "message": "Empty response"}
        
        checks = {
            "has_main_answer": self._has_main_answer(response),
            "has_context": self._has_context(response),
            "has_numbers": bool(re.search(r'\d+', response)),
            "no_truncation": not response.endswith('...'),
            "proper_ending": response.rstrip()[-1] in '.!?'
        }
        
        score = sum(checks.values()) / len(checks)
        
        return {
            "score": score,
            "checks": checks
        }
    
    def _has_main_answer(self, response: str) -> bool:
        """Check if response has a main answer"""
        # Look for definitive statements
        patterns = [r'is\s+[\$₹]?\d+', r'are\s+\d+', r'total.*[\$₹]?\d+']
        return any(re.search(p, response, re.IGNORECASE) for p in patterns)
    
    def _has_context(self, response: str) -> bool:
        """Check if response provides context"""
        context_words = ['because', 'due to', 'this means', 'indicating', 
                        'suggesting', 'shows that', 'based on']
        return any(word in response.lower() for word in context_words)


class EvaluationFramework:
    """Main evaluation framework combining all evaluators"""
    
    def __init__(self):
        self.query_evaluator = QueryEvaluator()
        self.faithfulness_evaluator = FaithfulnessEvaluator()
        self.relevance_evaluator = RelevanceEvaluator()
        self.completeness_evaluator = CompletenessEvaluator()
        self.evaluation_history: List[EvaluationResult] = []
    
    def evaluate_response(self, question: str, sql: str, result_df: pd.DataFrame,
                          response: str, facts: List[Dict]) -> EvaluationResult:
        """
        Comprehensive evaluation of a response
        
        Args:
            question: Original user question
            sql: Generated SQL query
            result_df: Query result DataFrame
            response: Final response text
            facts: Extracted facts from data
            
        Returns:
            Complete evaluation result
        """
        # Find matching test case or use defaults
        test_case = self._find_matching_test_case(question)
        
        # Evaluate each dimension
        sql_eval = self.query_evaluator.evaluate_sql(sql, test_case) if test_case else {"score": 0.5}
        result_eval = self.query_evaluator.evaluate_result(result_df, test_case) if test_case else {"score": 0.5}
        
        faithfulness_eval = self.faithfulness_evaluator.evaluate(response, facts, result_df)
        relevance_eval = self.relevance_evaluator.evaluate(question, response)
        completeness_eval = self.completeness_evaluator.evaluate(question, response)
        
        # Calculate accuracy as combination of SQL and result evaluation
        accuracy = (sql_eval.get("score", 0.5) + result_eval.get("score", 0.5)) / 2
        
        # Overall weighted score
        overall = (
            accuracy * 0.25 +
            faithfulness_eval["score"] * 0.30 +
            relevance_eval["score"] * 0.25 +
            completeness_eval["score"] * 0.20
        )
        
        result = EvaluationResult(
            accuracy_score=accuracy,
            faithfulness_score=faithfulness_eval["score"],
            relevance_score=relevance_eval["score"],
            completeness_score=completeness_eval["score"],
            overall_score=overall,
            details={
                "sql_evaluation": sql_eval,
                "result_evaluation": result_eval,
                "faithfulness": faithfulness_eval,
                "relevance": relevance_eval,
                "completeness": completeness_eval
            }
        )
        
        self.evaluation_history.append(result)
        return result
    
    def _find_matching_test_case(self, question: str) -> Optional[TestCase]:
        """Find a test case that matches the question type"""
        question_lower = question.lower()
        
        for test_case in self.query_evaluator.test_cases:
            # Simple keyword matching
            test_keywords = test_case.question.lower().split()
            question_keywords = question_lower.split()
            
            matches = sum(1 for kw in test_keywords if kw in question_keywords)
            if matches >= 2:
                return test_case
        
        return None
    
    def get_average_scores(self) -> Dict[str, float]:
        """Get average scores across all evaluations"""
        if not self.evaluation_history:
            return {"message": "No evaluations yet"}
        
        n = len(self.evaluation_history)
        return {
            "accuracy": sum(e.accuracy_score for e in self.evaluation_history) / n,
            "faithfulness": sum(e.faithfulness_score for e in self.evaluation_history) / n,
            "relevance": sum(e.relevance_score for e in self.evaluation_history) / n,
            "completeness": sum(e.completeness_score for e in self.evaluation_history) / n,
            "overall": sum(e.overall_score for e in self.evaluation_history) / n,
            "total_evaluations": n
        }
    
    def run_benchmark(self, agent_func) -> Dict[str, Any]:
        """
        Run benchmark on standard test cases
        
        Args:
            agent_func: Function that takes question and returns response
            
        Returns:
            Benchmark results
        """
        results = []
        
        for test_case in self.query_evaluator.test_cases:
            try:
                response = agent_func(test_case.question)
                # Would need full pipeline info for complete evaluation
                results.append({
                    "question": test_case.question,
                    "passed": True,
                    "response_preview": response[:100] if response else None
                })
            except Exception as e:
                results.append({
                    "question": test_case.question,
                    "passed": False,
                    "error": str(e)
                })
        
        passed = sum(1 for r in results if r["passed"])
        
        return {
            "total": len(results),
            "passed": passed,
            "failed": len(results) - passed,
            "pass_rate": passed / len(results) if results else 0,
            "details": results
        }


# Singleton instance
_evaluation_framework: Optional[EvaluationFramework] = None


def get_evaluation_framework() -> EvaluationFramework:
    """Get singleton evaluation framework instance"""
    global _evaluation_framework
    if _evaluation_framework is None:
        _evaluation_framework = EvaluationFramework()
    return _evaluation_framework
